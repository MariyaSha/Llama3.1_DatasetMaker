{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama3.1 Dataset Maker for Nvidia NIM\n",
    "\n",
    "This notebook is intended to run in a Llama3.1 NIM environment.\n",
    "<br>\n",
    "To set it up, please watch my video tutorial on the topic.\n",
    "\n",
    "## Setup NGC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "wget https://raw.githubusercontent.com/brevdev/notebooks/main/assets/setup-ngc.sh -O setup-ngc\n",
    "chmod +x setup-ngc\n",
    "./setup-ngc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Llama3.1 NIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "export NGC_API_KEY= # paste your NGC API key here\n",
    "\n",
    "# Log in to NGC\n",
    "echo \"${NGC_API_KEY}\" | docker login nvcr.io -u '$oauthtoken' --password-stdin\n",
    "\n",
    "# Set up NIM cache directory\n",
    "mkdir -p $HOME/.nim-cache\n",
    "\n",
    "docker run -d --rm --name=\"llama\" \\\n",
    "    --network=container:verb-workspace \\\n",
    "    --runtime=nvidia \\\n",
    "    --gpus all \\\n",
    "    --shm-size=16GB \\\n",
    "    -e NGC_API_KEY \\\n",
    "    -v $HOME/.nim-cache:/home/user/.nim-cache \\\n",
    "    -v /home/ubuntu/workspace:/workspace \\\n",
    "    -w /workspace \\\n",
    "    nvcr.io/nim/meta/llama-3.1-8b-instruct:1.1.0\n",
    "\n",
    "# Check if NIM is up\n",
    "echo \"Checking if NIM is up...\"\n",
    "while true; do\n",
    "    if curl -s http://localhost:8000 > /dev/null; then\n",
    "        echo \"NIM has been started successfully!\"\n",
    "        break\n",
    "    else\n",
    "        echo \"NIM is not up yet. Checking again in 10 seconds...\"\n",
    "        sleep 10\n",
    "    fi\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify that Llama is Available\n",
    "Specify the client URL to check if Llama3.1 is listening on port 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl localhost:8000/v1/health/ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset\n",
    "\n",
    "In the following cell, you will see how to chain prompt output to generate datasets automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "\n",
    "# create empty dataframe \n",
    "data = pd.DataFrame(columns=[\"country\", \"capital\", \"food\"])\n",
    "\n",
    "# specify model location\n",
    "client = OpenAI(\n",
    "  base_url = \"http://localhost:8000/v1\",\n",
    "  api_key = \"not_used\"\n",
    ")\n",
    "\n",
    "def ask_question(user_input):\n",
    "    \n",
    "    # specify model settings\n",
    "    chat_response = client.chat.completions.create(\n",
    "    model=\"meta/llama-3.1-8b-instruct\",\n",
    "    messages=[{\"role\":\"user\",\"content\": user_input}],\n",
    "    temperature=0.5,\n",
    "    top_p=1,\n",
    "    max_tokens=1024,\n",
    "    # return output as a single unit of text\n",
    "    stream=False\n",
    "    )\n",
    "\n",
    "    return chat_response.choices[0].message.content\n",
    "\n",
    "# fetch names of all world countries\n",
    "all_countries = ask_question(\"\"\"\n",
    "names of all countries separated by commas in an alphabetical order.\n",
    "names only, with no other output\n",
    "\"\"\")\n",
    "all_countries = all_countries.split(\", \")\n",
    "\n",
    "# iterate over all country names\n",
    "for i, country in enumerate(all_countries):\n",
    "    # fetch attributes for each country\n",
    "    capital = ask_question(\"what is the capital city of \" + country + \". just the name\")\n",
    "    food = ask_question(\"what is the national food of \" + country + \". just the name\")\n",
    "    # store country and attributes in the pre-defined dataframe\n",
    "    data.loc[i] = [country, capital, food]\n",
    "\n",
    "# save CSV file in the current directory\n",
    "data.to_csv(\"data.csv\", header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow: All Countries Output\n",
    "\n",
    "The next cell was used to customize the all_countries output from the cell above. You don't have to run it as it is already included in the dataset maker code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_countries = ask_question(\"\"\"\n",
    "names of all countries separated by commas in an alphabetical order.\n",
    "names only, with no other output\n",
    "\"\"\")\n",
    "all_countries = all_countries.split(\", \")\n",
    "print(all_countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
