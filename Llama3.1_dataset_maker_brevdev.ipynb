{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama3.1 Dataset Maker for Nvidia NIM\n",
    "\n",
    "This notebook is intended to run in a Llama3.1 NIM environment via Brev.dev.\n",
    "<br>\n",
    "Please make sure to **paste your NGC API KEY**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Llama3.1 NIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "# Install Docker CLI\n",
    "echo \"Installing Docker CLI...\"\n",
    "sudo apt-get update\n",
    "sudo apt-get install -y \\\n",
    "    apt-transport-https \\\n",
    "    ca-certificates \\\n",
    "    curl \\\n",
    "    gnupg \\\n",
    "    lsb-release\n",
    "curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\n",
    "echo \\\n",
    "  \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\\n",
    "  $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null\n",
    "sudo apt-get update\n",
    "sudo apt-get install -y docker-ce-cli\n",
    "\n",
    "#################################################\n",
    "# setup Llama3.1 NIM\n",
    "export NGC_API_KEY= # paste your NGC API KEY here\n",
    "#################################################\n",
    "\n",
    "# Log in to NGC\n",
    "echo \"${NGC_API_KEY}\" | docker login nvcr.io -u '$oauthtoken' --password-stdin\n",
    "\n",
    "# Set path to your LoRA model store\n",
    "export LOCAL_PEFT_DIRECTORY=\"$(pwd)/loras\"\n",
    "mkdir -p $LOCAL_PEFT_DIRECTORY\n",
    "pushd $LOCAL_PEFT_DIRECTORY\n",
    "popd\n",
    "\n",
    "chmod -R 777 $LOCAL_PEFT_DIRECTORY\n",
    "\n",
    "# Set up NIM cache directory\n",
    "mkdir -p $HOME/.nim-cache\n",
    "\n",
    "export NIM_PEFT_SOURCE=/workspace/loras # Path to LoRA models internal to the container\n",
    "export CONTAINER_NAME=meta-llama3_1-8b-instruct\n",
    "export NIM_CACHE_PATH=$HOME/.nim-cache\n",
    "export NIM_PEFT_REFRESH_INTERVAL=60\n",
    "\n",
    "docker run -d --name=$CONTAINER_NAME \\\n",
    "    --network=container:verb-workspace \\\n",
    "    --runtime=nvidia \\\n",
    "    --gpus all \\\n",
    "    --shm-size=16GB \\\n",
    "    -e NGC_API_KEY \\\n",
    "    -e NIM_PEFT_SOURCE \\\n",
    "    -e NIM_PEFT_REFRESH_INTERVAL \\\n",
    "    -v $HOME/.nim-cache:/home/user/.nim-cache \\\n",
    "    -v /home/ubuntu/workspace:/workspace \\\n",
    "    -w /workspace \\\n",
    "    nvcr.io/nim/meta/llama-3.1-8b-instruct:1.1.0\n",
    "\n",
    "# Check if NIM is up\n",
    "echo \"Checking if NIM is up...\"\n",
    "while true; do\n",
    "    if curl -s http://localhost:8000 > /dev/null; then\n",
    "        echo \"NIM has been started successfully!\"\n",
    "        break\n",
    "    else\n",
    "        echo \"NIM is not up yet. Checking again in 10 seconds...\"\n",
    "        sleep 10\n",
    "    fi\n",
    "done\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify that Llama is Available\n",
    "Specify the client URL to check if Llama3.1 is listening on port 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl localhost:8000/v1/health/ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install OpenAI\n",
    "To run the dataset maker, the openai modeule is required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset\n",
    "\n",
    "In the following cell, you will see how to chain prompt output to generate datasets automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "\n",
    "# create empty dataframe \n",
    "data = pd.DataFrame(columns=[\"country\", \"capital\", \"food\"])\n",
    "\n",
    "# specify model location\n",
    "client = OpenAI(\n",
    "  base_url = \"http://localhost:8000/v1\",\n",
    "  api_key = \"not_used\"\n",
    ")\n",
    "\n",
    "def ask_question(user_input):\n",
    "    \n",
    "    # specify model settings\n",
    "    chat_response = client.chat.completions.create(\n",
    "    # please note: model name written without a dot symbol in this implementation\n",
    "    model=\"meta/llama-3_1-8b-instruct\",\n",
    "    messages=[{\"role\":\"user\",\"content\": user_input}],\n",
    "    temperature=0.5,\n",
    "    top_p=1,\n",
    "    max_tokens=1024,\n",
    "    # return output as a single unit of text\n",
    "    stream=False\n",
    "    )\n",
    "\n",
    "    return chat_response.choices[0].message.content\n",
    "\n",
    "# fetch names of all world countries\n",
    "all_countries = ask_question(\"\"\"\n",
    "names of all countries separated by commas in an alphabetical order.\n",
    "names only, with no other output\n",
    "\"\"\")\n",
    "all_countries = all_countries.split(\", \")\n",
    "\n",
    "# iterate over all country names\n",
    "for i, country in enumerate(all_countries):\n",
    "    # fetch attributes for each country\n",
    "    capital = ask_question(\"what is the capital city of \" + country + \". just the name\")\n",
    "    food = ask_question(\"what is the national food of \" + country + \". just the name\")\n",
    "    # store country and attributes in the pre-defined dataframe\n",
    "    data.loc[i] = [country, capital, food]\n",
    "\n",
    "# save CSV file in the current directory\n",
    "data.to_csv(\"data.csv\", header=None)\n",
    "print(\"CSV file was successully saved in the root directory of your Jupyter Lab! :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
